{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Implementation of the character-level Elman RNN model.\n",
    "Written by Ngoc-Quan Pham based on Andreij Karparthy's lecture Cs231n.\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "import sys\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "# Since numpy doesn't have a function for sigmoid\n",
    "# We implement it manually here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def dsigmoid(y):\n",
    "    #derivative is sigmoid(x) * (1 - sigmoid(x))\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "# The derivative of the tanh function\n",
    "def dtanh(x):\n",
    "    return 1 - x * x\n",
    "\n",
    "\n",
    "# The numerically stable softmax implementation\n",
    "def softmax(x):\n",
    "    # assuming x shape is [feature_size, batch_size]\n",
    "    e_x = np.exp(x - np.max(x, axis=0)) #axis = 0 -> Spalten werden betrachtet und davon jeweils der größte Wert ausgegeben, spalte = feature vektor\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model parameters\n",
    "# Here we initialize the parameters based an random uniform distribution, with the std of 0.01\n",
    "\n",
    "# word embedding: each character in the vocabulary is mapped to a vector with $emb_size$ neurons\n",
    "# Transform one-hot vectors to embedding X\n",
    "Wex = np.random.randn(emb_size, vocab_size) * std\n",
    "\n",
    "# weight to transform input X to hidden H\n",
    "Wxh = np.random.randn(hidden_size, emb_size) * std\n",
    "\n",
    "# weight to transform previous hidden states H_{t-1} to hidden H_t\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * std  # hidden to hidden\n",
    "\n",
    "# Output layer: transforming the hidden states H to output layer\n",
    "Why = np.random.randn(vocab_size, hidden_size) * std  # hidden to output\n",
    "\n",
    "# The biases are typically initialized as zeros. But sometimes people init them with uniform distribution too.\n",
    "bh = np.random.randn(hidden_size, 1) * std  # hidden bias\n",
    "by = np.random.randn(vocab_size, 1) * std  # hidden bias\n",
    "\n",
    "# These variables are momentums for the Adagrad algorithm\n",
    "# Each parameter in the network needs one momentum correspondingly\n",
    "mWex, mWxh, mWhh, mWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "\n",
    "\n",
    "def forward(inputs, labels, memory, batch_size=1):\n",
    "    prev_h = memory\n",
    "    \"\"\"\n",
    "    # here we use dictionaries to store the activations over time\n",
    "    # note from back-propagation implementation:\n",
    "    # back-propagation uses dynamic programming to estimate gradients efficiently\n",
    "    # so we need to store the activations over the course of the forward pass\n",
    "    # in the backward pass we will use the activations to compute the gradients\n",
    "    # (otherwise we will need to recompute them)\n",
    "    \"\"\"\n",
    "\n",
    "    # those variables stand for:\n",
    "    # xs: inputs to the RNNs at timesteps (embeddings)\n",
    "    # cs: characters at timesteps\n",
    "    # hs: hidden states at timesteps\n",
    "    # ys: output layers at timesteps\n",
    "    # ps: probability distributions at timesteps\n",
    "    xs, cs, hs, os, ps, ys = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    # the first memory (before training) is the previous (or initial) hidden state\n",
    "    hs[-1] = np.copy(prev_h)\n",
    "\n",
    "    # the loss will be accumulated over time\n",
    "    loss = 0\n",
    "\n",
    "    # Inputs shape = batchsize, featuresize\n",
    "    for t in range(inputs.shape[1]): # for every feature bzw. Eingabe zum Zeitpunkt t\n",
    "\n",
    "        # one-hot vector representation for character input at time t\n",
    "        cs[t] = np.zeros((vocab_size, batch_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            cs[t][inputs[b][t]][b] = 1\n",
    "\n",
    "        # transform the one hot vector to embedding\n",
    "        # x = Wemb x c\n",
    "        xs[t] = np.dot(Wex, cs[t])\n",
    "\n",
    "        # computation for the hidden state of the network\n",
    "        # H = tanh ( Wh . H + Wx . x )\n",
    "        h_pre_activation = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh\n",
    "        hs[t] = np.tanh(h_pre_activation)\n",
    "\n",
    "        # output layer:\n",
    "        # this is the unnormalized log probabilities for next chars (across all chars in the vocabulary)\n",
    "        os[t] = np.dot(Why, hs[t]) + by\n",
    "\n",
    "        # softmax layer to get normalized probabilities:\n",
    "        ps[t] = softmax(os[t])\n",
    "\n",
    "        # the label is also an one-hot vector\n",
    "        ys[t] = np.zeros((vocab_size, batch_size))\n",
    "        for b in range(batch_size):\n",
    "            ys[t][labels[b][t]][b] = 1\n",
    "\n",
    "        # cross entropy loss at time t:\n",
    "        loss_t = np.sum(-np.log(ps[t]) * ys[t])\n",
    "\n",
    "        loss += loss_t\n",
    "\n",
    "    # packaging the activations to use in the backward pass\n",
    "    activations = (xs, cs, hs, os, ps, ys)\n",
    "    last_hidden = hs[inputs.shape[1] - 1]\n",
    "    return loss, activations, last_hidden\n",
    "\n",
    "\n",
    "def backward(activations, clipping=True, scale=True):\n",
    "    \"\"\"\n",
    "    during the backward pass we follow the track of the forward pass\n",
    "    the activations are needed so that we can avoid unnecessary re-computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Gradient initialization\n",
    "    # Each parameter has a corresponding gradient (of the loss with respect to that gradient)\n",
    "    dWex, dWxh, dWhh, dWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "    xs, cs, hs, os, ps, ys = activations\n",
    "\n",
    "    # here we need the gradient w.r.t to the hidden layer at the final time step\n",
    "    # since this hidden layer is not connected to any future (final time step)\n",
    "    # then we can initialize it as zero vectors\n",
    "    dh = np.zeros_like(hs[0])\n",
    "    bsz = dh.shape[-1]\n",
    "\n",
    "    # the backward pass starts from the final step of the chain in the forward pass\n",
    "    for t in reversed(range(inputs.shape[1])):\n",
    "\n",
    "        # first, we need to compute the gradients of the variable closest to the loss function,\n",
    "        # which is the softmax output p\n",
    "        # but here I skip it directly to the gradients of the unnormalized scores o because\n",
    "        # basically dL / do = p - y\n",
    "        # from the cross entropy gradients. (the explanation is a bit too long to write here)\n",
    "        do = ps[t] - ys[t]\n",
    "\n",
    "        if scale:\n",
    "            do = do / bsz\n",
    "\n",
    "        # the gradients w.r.t to the weights and the bias that were used to create o[t]\n",
    "        dWhy += np.dot(do, hs[t].T)\n",
    "        dby += np.sum(do, axis=-1, keepdims=True)\n",
    "\n",
    "        # because h is connected to both o and the next h, we sum the gradients up\n",
    "        dh = np.dot(Why.T, do) + dh\n",
    "\n",
    "        # backprop through the activation function (tanh)\n",
    "        dtanh_h = 1 - hs[t] * hs[t]\n",
    "        dh_pre_activation = dtanh_h * dh  # because h = tanh(h_pre_activation)\n",
    "\n",
    "        # next, since  H = tanh ( Wh . H + Wx . x + bh )\n",
    "        # we use dh to backprop to dWh and dWx\n",
    "\n",
    "        # gradient of the bias and weight, this is similar to dby and dWhy\n",
    "        # for the H term\n",
    "        dbh += np.sum(dh_pre_activation, axis=-1, keepdims=True)\n",
    "        dWhh += np.dot(dh_pre_activation, hs[t - 1].T)\n",
    "        # we need this term for the recurrent connection (previous bptt step needs this)\n",
    "        dh = np.dot(Whh.T, dh_pre_activation)\n",
    "\n",
    "        # similarly for the x term\n",
    "        dWxh += np.dot(dh_pre_activation, xs[t].T)\n",
    "\n",
    "        # backward through the embedding\n",
    "        dx = np.dot(Wxh.T, dh_pre_activation)\n",
    "\n",
    "        # finally backward to the embedding projection\n",
    "        dWex += np.dot(dx, cs[t].T)\n",
    "\n",
    "    if clipping:\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    gradients = (dWex, dWxh, dWhh, dWhy, dbh, dby)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    c = np.zeros((vocab_size, 1))\n",
    "    c[seed_ix] = 1\n",
    "    generated_chars = []\n",
    "    for t in range(n):\n",
    "        x = np.dot(Wex, c)\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        o = np.dot(Why, h) + by\n",
    "        p = softmax(o)\n",
    "\n",
    "        # the the distribution, we randomly generate samples:\n",
    "        ix = np.random.multinomial(1, p.ravel())\n",
    "        c = np.zeros((vocab_size, 1))\n",
    "\n",
    "        for j in range(len(ix)):\n",
    "            if ix[j] == 1:\n",
    "                index = j\n",
    "        c[index] = 1\n",
    "        generated_chars.append(index)\n",
    "\n",
    "    return generated_chars\n",
    "\n",
    "\n",
    "option = sys.argv[1]  # train or gradcheck\n",
    "\n",
    "if option == 'train':\n",
    "\n",
    "    n, p = 0, 0\n",
    "    data_length = cut_stream.shape[1]\n",
    "\n",
    "    # I am not perfectly sure about this (learnt from others that the initial \"perplexity\" of the model\n",
    "    # should be the vocabulary for every position. So this is the loss at iteration 0\n",
    "    smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "    while True:\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if p + seq_length + 1 >= data_length or n == 0:\n",
    "            hprev = np.zeros((hidden_size, batch_size))  # reset RNN memory\n",
    "            p = 0  # go back to start of data\n",
    "\n",
    "        inputs = cut_stream[:, p:p + seq_length]\n",
    "        targets = cut_stream[:, p + 1:p + 1 + seq_length]\n",
    "\n",
    "        # sample from the model now and then\n",
    "        if n % 1000 == 0:\n",
    "            h_zero = np.zeros((hidden_size, 1))\n",
    "            sample_ix = sample(h_zero, inputs[0][0], 1500)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "        # forward seq_length characters through the net and fetch gradient\n",
    "        loss, activations, hprev = forward(inputs, targets, hprev, batch_size=batch_size)\n",
    "        gradients = backward(activations)\n",
    "        dWex, dWxh, dWhh, dWhy, dbh, dby = gradients\n",
    "        smooth_loss = smooth_loss * 0.999 + loss / batch_size * 0.001\n",
    "        if n % 20 == 0:\n",
    "            print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wex, Wxh, Whh, Why, bh, by],\n",
    "                                      [dWex, dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                      [mWex, mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "        p += seq_length  # move data pointer\n",
    "        n += 1  # iteration counter\n",
    "\n",
    "elif option == 'gradcheck':\n",
    "\n",
    "    p = 0\n",
    "    inputs = cut_stream[:, p:p + seq_length]\n",
    "    targets = cut_stream[:, p + 1:p + 1 + seq_length]\n",
    "\n",
    "    delta = 0.0001\n",
    "\n",
    "    hprev = np.zeros((hidden_size, batch_size))\n",
    "    memory = hprev\n",
    "\n",
    "    loss, activations, memory = forward(inputs, targets, hprev, batch_size=batch_size)\n",
    "    # for gradient-checking we don't clip the gradients\n",
    "    gradients = backward(activations, clipping=False, scale=False)\n",
    "    dWex, dWxh, dWhh, dWhy, dbh, dby = gradients\n",
    "\n",
    "    for weight, grad, name in zip([Wex, Wxh, Whh, Why, bh, by],\n",
    "                                  [dWex, dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  ['dWex', 'dWxh', 'dWhh', 'dWhhy', 'dbh', 'dby']):\n",
    "\n",
    "        str_ = (\"Dimensions dont match between weight and gradient %s and %s.\" % (weight.shape, grad.shape))\n",
    "        assert (weight.shape == grad.shape), str_\n",
    "\n",
    "        count_idx = 0\n",
    "        grad_num_sum = 0\n",
    "        grad_ana_sum = 0\n",
    "        rel_error_sum = 0\n",
    "        error_idx = []\n",
    "\n",
    "        print(name)\n",
    "        for i in range(weight.size):\n",
    "\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            w = weight.flat[i]\n",
    "            weight.flat[i] = w + delta\n",
    "            loss_positive, _, _ = forward(inputs, targets, hprev, batch_size=batch_size)\n",
    "            weight.flat[i] = w - delta\n",
    "            loss_negative, _, _ = forward(inputs, targets, hprev, batch_size=batch_size)\n",
    "            weight.flat[i] = w  # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = grad.flat[i]\n",
    "            grad_numerical = (loss_positive - loss_negative) / (2 * delta)\n",
    "            grad_num_sum += grad_numerical\n",
    "            grad_ana_sum += grad_analytic\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "            if rel_error is None:\n",
    "                rel_error = 0.\n",
    "            rel_error_sum += rel_error\n",
    "\n",
    "            if rel_error > 0.0001:\n",
    "                print('WARNING %f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "                count_idx += 1\n",
    "                error_idx.append(i)\n",
    "\n",
    "        print('For %s found %i bad gradients; with %i total parameters in the vector/matrix!' % (\n",
    "            name, count_idx, weight.size))\n",
    "        print(' Average numerical grad: %0.9f \\n Average analytical grad: %0.9f \\n Average relative grad: %0.9f' % (\n",
    "            grad_num_sum / float(weight.size), grad_ana_sum / float(weight.size), rel_error_sum / float(weight.size)))\n",
    "        print(' Indizes at which analytical gradient does not match numerical:', error_idx)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\n",
    "from icecream import ic\n",
    "\n",
    "class dataset:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # data I/O\n",
    "        # should be simple plain text file. The sample from \"Hamlet - Shakespeares\" is provided in data/\n",
    "        data = open('data/input.txt', 'r').read()\n",
    "        chars = sorted(list(set(data)))  # added sorted so that the character list is deterministic\n",
    "        print(chars)\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "        ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        # this will load the data into memory\n",
    "        self.data_stream = np.asarray([char_to_ix[char] for char in data])\n",
    "        print(self.data_stream.shape)\n",
    "        data.close()\n",
    "\n",
    "\n",
    "    def get_cut_stream(self,seq_length, batch_size):\n",
    "        bound = (self.data_stream.shape[0] // (seq_length * batch_size)) * (seq_length * batch_size)\n",
    "        cut_stream = self.data_stream[:bound]\n",
    "        cut_stream = np.reshape(cut_stream, (batch_size, -1))\n",
    "\n",
    "        return cut_stream\n",
    "\n",
    "# hyper-parameters deciding the network size\n",
    "emb_size = 4  # word/character embedding size\n",
    "seq_length = 32  # number of steps to unroll the RNN for the truncated back-propagation algorithm\n",
    "hidden_size = 32\n",
    "# learning rate for the Adagrad algorithm. (this one is not 'optimized', only required to make the model learn)\n",
    "learning_rate = 0.02\n",
    "std = 0.02  # The standard deviation for parameter initilization\n",
    "batch_size = 4\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1115394,)\n",
      "1115392\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cut_stream.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 278848)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "p = 0\n",
    "inputs = cut_stream[:, p:p + seq_length]\n",
    "targets = cut_stream[:, p + 1:p + 1 + seq_length]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14,\n",
       "        43, 44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42],\n",
       "       [ 1, 46, 53, 61,  1, 58, 53,  1, 41, 59, 56, 57, 43,  8,  0,  0,\n",
       "        29, 33, 17, 17, 26,  1, 17, 24, 21, 38, 13, 14, 17, 32, 20, 10],\n",
       "       [43,  1, 44, 50, 39, 58, 58, 43, 56, 47, 52, 45,  1, 58, 56, 59,\n",
       "        58, 46,  1, 53, 44,  1, 57, 50, 43, 43, 54,  6,  0, 25, 63,  1],\n",
       "       [50, 50,  1, 53, 52, 43,  1, 58, 53,  1, 51, 43, 11,  1, 44, 53,\n",
       "        56,  1, 46, 39, 42,  1, 21,  0, 40, 43, 43, 52,  1, 58, 46, 43]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('imgcrop')"
  },
  "interpreter": {
   "hash": "043c6cbfee877234194bbdeb3904ba83d0fa911149e24701c48713c6a0a5c454"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}